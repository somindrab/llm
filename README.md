# LLM
An implementation of OpenAI's GPT-2 model from scratch.

The jupyter notebooks have implementations of the different building blocks to experiment.

All the python code is in gpt/

The directory named "training" has code to train the model on Project Gutenberg's books (you'll have to download Project Gutenberg's dataset; not included here).

training/training_logs contains the logs from various runs. Hyperparameters are also logged. As are the training and validation losses. TODO: upload the plots.

The Jupyter notebooks have extensive commentary; perhaps a little too much. 

