{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936cbede-187f-44cf-9f70-3fa12bd70531",
   "metadata": {},
   "source": [
    "In simplified attention weights, we essentially went ahead and use the fact that the embedding layer already has vectors pointing in the right directions and so we can figure out the \"alignment\" between the tokens.\n",
    "\n",
    "But remember that we want to keep \"an open mind\". That embedded layer is a result of the training that is done, and therefore the tokens' alignments are representative of the type and quality of the training data.\n",
    "\n",
    "What we really want to do is train our neural net with data specific for our domain or use case. Which means that as we feed more data for our use case during training, we want these attention weights to be trainable too.\n",
    "\n",
    "Remember that we are talking about the training the attention weights. We are not (yet) talking about updating the embedding layer vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95739e89-25ef-4eb5-a526-bb9d871e613f",
   "metadata": {},
   "source": [
    "To do so, we are going to make use of three weight matrices.\n",
    "Key, Query, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82c1c8ee-cafd-4261-afda-bd2861d7f791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "d_in = inputs.shape[1] ## the input embedding size\n",
    "d_out = 2 ## the output embedding size\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97e55228-7149-4d4e-a39c-3b7ef3bee105",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "#Think of torch.nn.Parameter as a multidimensional matrix, for now.\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe4a1718-1aba-42a7-b851-8c755a526557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#project the embedded vectors over these vectors. Remember that these random but the whole idea is that we can\n",
    "#train them during training.\n",
    "\n",
    "keys = inputs@W_key\n",
    "values=inputs@W_value\n",
    "queries = inputs@W_query\n",
    "\n",
    "#embedding dimension of the keys\n",
    "#remember that the first dimension is always the number of tokens. 6 for example here.\n",
    "#the second dimension is the number of dimensions in the keys that we have projected\n",
    "#the vectors over, i.e., we've embedded them over this dimension of the keys vectors.\n",
    "\n",
    "d_k = keys.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f989a44-bd30-4e6e-b40a-767fd5364b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = queries@keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3ff5ef8-5169-46c0-a55f-7e8b58bf079e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
       "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
       "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
       "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
       "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
       "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we are going to normalize but this time around, we are going to use \n",
    "#dot product scaling.\n",
    "#scaling by the square root of the embedding dimension is also why this self-attension\n",
    "#mechanism is called the scaled-dot product attention\n",
    "attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b17a6271-ab14-4882-a4cd-de2a6ca2e377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2996, 0.8053],\n",
       "        [0.3061, 0.8210],\n",
       "        [0.3058, 0.8203],\n",
       "        [0.2948, 0.7939],\n",
       "        [0.2927, 0.7891],\n",
       "        [0.2990, 0.8040]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = attn_weights @ values\n",
    "\n",
    "context_vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
