{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca78cfc0-6917-41db-a73f-938f1f67fed5",
   "metadata": {},
   "source": [
    "### Layer normalization\n",
    "Done to ensure that the outputs of a layer have a mean of 0 and a variance of 1. Helps with training stability. The input to a transformer block and its output are layer normalized. And so is the final output before the token decoding in GPT.\n",
    "\n",
    "But take all this with a pinch of salt. We want to use layer normalization (mean=0, variance=1) only if it really helps. What if it doesn't? That's the beauty of scale and shift below - these are trainable parameters that the model can update during training to adjust the layer norm output to best suit the training needs.\n",
    "\n",
    "Kinda beautiful if you think about it. Fence sitting. But still beautiful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61a6f8f9-35f8-4cbc-ab24-17951071b217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    0.0000],\n",
       "         [    0.0000]], grad_fn=<MeanBackward1>),\n",
       " tensor([[1.0000],\n",
       "         [1.0000]], grad_fn=<VarBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    #embedded_dim represents the number of embedded dimensions\n",
    "    def __init__(self, embedded_dim):\n",
    "        super().__init__()\n",
    "        self.epsilon = 1e-5 #see below. avoids a division by 0.\n",
    "        self.scale = nn.Parameter(torch.ones(embedded_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(embedded_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm = (x - mean)/torch.sqrt(var + self.epsilon)\n",
    "        scaled_shifted_norm = (norm * self.scale) + self.shift\n",
    "        return scaled_shifted_norm\n",
    "\n",
    "\n",
    "batch = torch.randn(2, 5)\n",
    "l = LayerNorm(batch.shape[-1])\n",
    "out = l(batch)\n",
    "out\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "out.mean(dim=-1, keepdim=True), out.var(dim=-1, keepdim=True, unbiased=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34faade5-a531-4fbb-857c-be919186a321",
   "metadata": {},
   "source": [
    "### GELU activation function\n",
    "Gaussian error linear unit\n",
    "\n",
    "GELU(x) = 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3))\n",
    "\n",
    "Preferred over the ReLU. No elbows. Smoooooth. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2813a88b-3f0f-42c6-9f35-605c0e7170b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0/torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb5defb-87a8-498f-baf8-2f457917dce8",
   "metadata": {},
   "source": [
    "### Feed forward layer\n",
    "Now this one required understanding the GPT model as a whole after trying to train and use it. But think of it as an expansion in the dimensional space to give the model a chance to tease out connections in a much higher dimensional space than its embedding space, and then ofcourse back to the emdedding dimensions. The activation function here is GELU. The code is pretty straightforward but the concept is rich. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfd886a3-0c48-4e0a-ba0c-81e7fa979b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    # we are going to use a configuration dict here to avoid have to pass in random looking \n",
    "    # parameters\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ac70a-d46e-4361-9513-5f2952d7a231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
