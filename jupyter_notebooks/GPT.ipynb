{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca78cfc0-6917-41db-a73f-938f1f67fed5",
   "metadata": {},
   "source": [
    "### Layer normalization\n",
    "Done to ensure that the outputs of a layer have a mean of 0 and a variance of 1. Helps with training stability. The input to a transformer block and its output are layer normalized. And so is the final output before the token decoding in GPT.\n",
    "\n",
    "But take all this with a pinch of salt. We want to use layer normalization (mean=0, variance=1) only if it really helps. What if it doesn't? That's the beauty of scale and shift below - these are trainable parameters that the model can update during training to adjust the layer norm output to best suit the training needs.\n",
    "\n",
    "Kinda beautiful if you think about it. Fence sitting. But still beautiful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61a6f8f9-35f8-4cbc-ab24-17951071b217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    0.0000],\n",
       "         [    0.0000]], grad_fn=<MeanBackward1>),\n",
       " tensor([[1.0000],\n",
       "         [1.0000]], grad_fn=<VarBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    #embedded_dim represents the number of embedded dimensions\n",
    "    def __init__(self, embedded_dim):\n",
    "        super().__init__()\n",
    "        self.epsilon = 1e-5 #see below. avoids a division by 0.\n",
    "        self.scale = nn.Parameter(torch.ones(embedded_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(embedded_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm = (x - mean)/torch.sqrt(var + self.epsilon)\n",
    "        scaled_shifted_norm = (norm * self.scale) + self.shift\n",
    "        return scaled_shifted_norm\n",
    "\n",
    "\n",
    "batch = torch.randn(2, 5)\n",
    "l = LayerNorm(batch.shape[-1])\n",
    "out = l(batch)\n",
    "out\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "out.mean(dim=-1, keepdim=True), out.var(dim=-1, keepdim=True, unbiased=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34faade5-a531-4fbb-857c-be919186a321",
   "metadata": {},
   "source": [
    "### GELU activation function\n",
    "Gaussian error linear unit\n",
    "\n",
    "GELU(x) = 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3))\n",
    "\n",
    "Preferred over the ReLU. No elbows. Smoooooth. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2813a88b-3f0f-42c6-9f35-605c0e7170b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0/torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb5defb-87a8-498f-baf8-2f457917dce8",
   "metadata": {},
   "source": [
    "### Feed forward layer\n",
    "Now this one required understanding the GPT model as a whole after trying to train and use it. But think of it as an expansion in the dimensional space to give the model a chance to tease out connections in a much higher dimensional space than its embedding space, and then ofcourse back to the emdedding dimensions. The activation function here is GELU. The code is pretty straightforward but the concept is rich. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfd886a3-0c48-4e0a-ba0c-81e7fa979b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    # we are going to use a configuration dict here to avoid have to pass in random looking \n",
    "    # parameters\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac80ad2-fc45-4f3d-97bf-d8e8927877ca",
   "metadata": {},
   "source": [
    "##### Multihead Attention implemented previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7eb48f5e-d8ec-4786-a1fd-d76bc4c22475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "## So here, the d_out has to be a multiple of the number of heads.\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout,\n",
    "                 num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        #The number of heads per dimension\n",
    "        self.head_dim = self.d_out // self.num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        #print(f\"W_key weight shape: {self.W_key.weight.shape}\")\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        #print(f\"keys.shape: {keys.shape}\")\n",
    "        \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        \n",
    "        #print(f\"keys.shape after splitting into heads: {keys.shape}\")\n",
    "\n",
    "        #Now here, we are going to \"rearrange\" the matrices such that we go from\n",
    "        #[b, num_tokens, num_heads, head_dim] -> [b, num_heads, num_tokens, head_dim]\n",
    "        #and this should make a lot of sense because after all, we are trying to process\n",
    "        #multiple heads here, and so arranging this in that hierarchy is what we need to do\n",
    "\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        #and now really business as usual. We want to compute the attn weights and context\n",
    "        #vectors, just that we need to remain hyper aware that our matrices are hierarchically\n",
    "        #arranged as batches -> heads -> tokens -> embedded vectors\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        #scaled dot product\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        #apply the dropout if there is one specified\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        #calculate the context vectors\n",
    "        context_vectors = attn_weights @ values\n",
    "\n",
    "        #Now, we want to go back to how we desire the context vectors, i.e.,\n",
    "        #(b, num_tokens, vectors) where all of the vectors from the multiple heads are\n",
    "        #\"combined\" to see the uniform result of multihead processing.\n",
    "        #To get there, we need to start rearranging the hierarchy \n",
    "\n",
    "        context_vectors = context_vectors.transpose(1,2)\n",
    "        #so now this will become (b, tokens, heads, vectors)\n",
    "\n",
    "        context_vectors = context_vectors.contiguous().view(b, num_tokens, self.d_out)\n",
    "        #so now this will become (b, tokens, vectors)\n",
    "\n",
    "        #apply a linear projection that will be useful when this class is used in training\n",
    "        context_vectors = self.out_proj(context_vectors)\n",
    "\n",
    "        return context_vectors\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc08068-72a2-4d60-8853-b7da97baf9f3",
   "metadata": {},
   "source": [
    "### And now we actually have all the components we need to put together the transformer block\n",
    "\n",
    "Each Transformer block:\n",
    "\n",
    "-> Layer Norm -> Multihead attention -> Dropout -> Layer Norm -> Feed forward -> Dropout ->\n",
    "|                 (shortcut)                    |                  (shortcut)             |\n",
    "|-----------------------------------------------|-----------------------------------------|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eded9ed-1715-4538-840a-9d75a70c5e90",
   "metadata": {},
   "source": [
    "### A couple of points to remember...\n",
    "\n",
    "...because I know you will be confused O(months) from now when you look at this code again, Somindra.\n",
    "\n",
    "So, why do we need two normalizers, when all they do is exactly the same ... normalization. Remember that each of them has trainable weight parameters, which makes them position specific in the pipeline. Scale and Shift and all that.\n",
    "\n",
    "What's with the shortcut + x thing, if its a shortcut? The shortcut is taken during training and therefore backprop. So depending upon if the shortcut is chosen or not, we cannot backprop unless the input vectors have been .... added to the state of the vectors before the changes happen that we may skip over. The backprop won't work basically.... think physics again and not cs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7cf6873-10d3-4d3a-b8d6-3c9b8e9ac37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.attn = MultiHeadAttention(d_in = cfg[\"emb_dim\"],\n",
    "                                       d_out = cfg[\"emb_dim\"],\n",
    "                                       context_length = cfg[\"context_length\"],\n",
    "                                       dropout = cfg[\"drop_rate\"],\n",
    "                                       num_heads = cfg[\"n_heads\"],\n",
    "                                       qkv_bias = cfg[\"qkv_bias\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.layer_norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.feed_forward = FeedFoward(cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.feed_forward(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25c76ba5-9ac3-4df3-9e97-c43b4f931d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a2b0a5a-7096-4858-adfb-7530e1c19377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "### Lets try it out\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "\n",
    "x = torch.randn(2, 4, 768) #batch size 2, each with 4 input tokens, 768 embedded dimensions\n",
    "\n",
    "output = block(x)\n",
    "\n",
    "print(x.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e025c9-0802-458c-9186-438598ef7621",
   "metadata": {},
   "source": [
    "### And finally, we are in a position to code the GPT itself.\n",
    "\n",
    "Tokenized Text is fed into:\n",
    "\n",
    "Token embedding + Positional embedding -> Dropout -> Transformer Block 1 -> Transformer Block 2 -> ... -> Transformer Block 12 -> Final Layer Normalization -> Linear Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd4d9fa6-f032-4497-9a48-149b8e440cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModule(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.positional_embedding = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "                                *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding(in_idx)\n",
    "        pos_emb = self.positional_embedding(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.trf_blocks(x)\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        logits = self.out_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef4271e3-2bb1-49aa-a8d3-d137cfe294c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 50257])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's try this out\n",
    "torch.manual_seed(123)\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2))) #batch is a list\n",
    "batch = torch.stack(batch, dim=0) #converts from a list of two tensors to a tensor of shape 2,4\n",
    "\n",
    "gpt = GPTModule(GPT_CONFIG_124M)\n",
    "\n",
    "out = gpt(batch)\n",
    "\n",
    "out.shape #[2, 4, 50257] i.e. 2 batches, 4 tokens each, 50257 dimensions per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06a530a5-b499-49cc-a2b9-0fc605987a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        trimmed_idx = idx[:, -context_size:] #use only the last context_size as context if len(idx) > context_size\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(trimmed_idx)\n",
    "\n",
    "        logits = logits[:, -1, :] #we want only the last token\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "36cf692d-a374-4de2-89c6-def5539e2006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, I am Featureiman Byeswickattribute argue'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"Hello, I am\"\n",
    "\n",
    "encoded_context = tokenizer.encode(context)\n",
    "\n",
    "#print(encoded_context) #just a list\n",
    "\n",
    "encoded_context_tensor = torch.tensor(encoded_context)\n",
    "#print(encoded_context_tensor) #just a tensor with the same info as the list\n",
    "\n",
    "#we want this to look like (batch_size, tokens)\n",
    "encoded_context_tensor = encoded_context_tensor.unsqueeze(0)\n",
    "#print(encoded_context_tensor)\n",
    "#print(encoded_context_tensor.shape)\n",
    "\n",
    "gpt.eval()\n",
    "out = generate_text_simple(gpt, encoded_context_tensor, 6, context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "\n",
    "print(out)\n",
    "\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "\n",
    "decoded_text\n",
    "\n",
    "#'Hello, I am Featureiman Byeswickattribute argue'\n",
    "# classic\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
