{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0ad052-8495-471a-8f5f-1ef92c4a814b",
   "metadata": {},
   "source": [
    "A comparatively efficienct implementation of a multihead attention class with dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8284a8a-e319-4fdb-897f-ae991761116d",
   "metadata": {},
   "source": [
    "In the CausalAttention notebook, the last class implemented is a wrapper that takes multiple Causal Attention modules and then serially projects the input over the trainable weights to get context vectors, which are then stacked together in the output. This is way less efficient due to the number of matrix multiplications is equal to the number of heads. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bae608-4b8d-4db1-98b8-3b0780895757",
   "metadata": {},
   "source": [
    "Instead, we are going to create matrices that have the Q, K, V weights for all heads in them, multiply once (i.e. do the projections once), and then tease the output apart to get to context vectors from each head. Ofcourse, we will take this all the way this time around and combine the context vectors to represent the context vectors obtained via multihead attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e40a4e90-8d8e-4441-ac7e-07ee847f91ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "## So here, the d_out has to be a multiple of the number of heads.\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout,\n",
    "                 num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        #The number of heads per dimension\n",
    "        self.head_dim = self.d_out // self.num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        print(f\"W_key weight shape: {self.W_key.weight.shape}\")\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        print(f\"keys.shape: {keys.shape}\")\n",
    "        \n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        \n",
    "        print(f\"keys.shape after splitting into heads: {keys.shape}\")\n",
    "\n",
    "        #Now here, we are going to \"rearrange\" the matrices such that we go from\n",
    "        #[b, num_tokens, num_heads, head_dim] -> [b, num_heads, num_tokens, head_dim]\n",
    "        #and this should make a lot of sense because after all, we are trying to process\n",
    "        #multiple heads here, and so arranging this in that hierarchy is what we need to do\n",
    "\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        #and now really business as usual. We want to compute the attn weights and context\n",
    "        #vectors, just that we need to remain hyper aware that our matrices are hierarchically\n",
    "        #arranged as batches -> heads -> tokens -> embedded vectors\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        #scaled dot product\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        #apply the dropout if there is one specified\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        #calculate the context vectors\n",
    "        context_vectors = attn_weights @ values\n",
    "\n",
    "        #Now, we want to go back to how we desire the context vectors, i.e.,\n",
    "        #(b, num_tokens, vectors) where all of the vectors from the multiple heads are\n",
    "        #\"combined\" to see the uniform result of multihead processing.\n",
    "        #To get there, we need to start rearranging the hierarchy \n",
    "\n",
    "        context_vectors = context_vectors.transpose(1,2)\n",
    "        #so now this will become (b, tokens, heads, vectors)\n",
    "\n",
    "        context_vectors = context_vectors.contiguous().view(b, num_tokens, self.d_out)\n",
    "        #so now this will become (b, tokens, vectors)\n",
    "\n",
    "        #apply a linear projection that will be useful when this class is used in training\n",
    "        context_vectors = self.out_proj(context_vectors)\n",
    "\n",
    "        return context_vectors\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d909368-5aad-4746-9504-709cb0e738ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024, 800])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch size, context length, number of embedded dimensions\n",
    "batch = torch.randn(8, 1024, 800)\n",
    "d_in = batch.shape[2] ## the input embedding size\n",
    "d_out = 400 ## the output embedding size\n",
    "\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "658aaa76-46cc-4055-888b-988e7edaef44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_key weight shape: torch.Size([400, 800])\n",
      "keys.shape: torch.Size([8, 1024, 400])\n",
      "keys.shape after splitting into heads: torch.Size([8, 1024, 2, 200])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1024, 400])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(d_in, d_out, batch.shape[1], 0, 2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "context_vecs.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
